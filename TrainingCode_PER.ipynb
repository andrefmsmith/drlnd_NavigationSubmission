{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Navigation\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Instantiate the Unity Environment for Project 1 (vector observation version of 'Banana'), retrieve the Unity brain name and extract the environment's state size (```s_sz```) and action size (```a_sz```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# start Unity environment\n",
    "env = UnityEnvironment(file_name=\"C:/Users/Andre/Desktop/Udacity DRL/deep-reinforcement-learning/p1_navigation/Banana_Windows_x86_64/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# get state size and action size\n",
    "#env_info = env.reset(train_mode=False)[brain_name]\n",
    "s_sz = brain.vector_observation_space_size# len(env_info.vector_observations[0])\n",
    "a_sz = brain.vector_action_space_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNetwork class\n",
    "Define the QNetwork object class, which will instantiate the Agent's neural networks for learning and setting targets for learning of action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=64):\n",
    "        '''Creates a 2-hidden layer neural network with input and output size defined by\n",
    "        state_size and output_size respectively. Hidden layer width is configured by fc1_units and fc2_units.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . state_size (int): size of state space\n",
    "        . action_size (int): size of action space\n",
    "            '''\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        '''Performs forward propagation on the QNetwork given a state input, to return action values.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . state (torch float tensor): current state to obtain action values for.'''\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class\n",
    "Define an Agent class containing two instances of QNetwork (a local and target q network, to enable Fixed Q Targets), a memory buffer (to enable Experience Replay), and methods/attributes to save experience to memory, control learning to occur every X time-steps, select epsilon-greedy actions, update weights for the local q network and perform cloning/soft updating of the target q network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        '''Instantiates an Agent for Deep Q Networks.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . state_size (int): size of state space\n",
    "        . action_size (int): size of action space\n",
    "        \n",
    "        Attributes\n",
    "        =========\n",
    "        . state_size (int): size of state space.\n",
    "        . action_size (int): size of action space.\n",
    "        . qnetwork_local (instance of QNetwork class): local q network for learning action values.\n",
    "        . qnetwork_target (instance of QNetwork class: target q network for directing learning.\n",
    "        . optimizer: which optimizer to be used from torch.optim package.\n",
    "        . memory (instance of ReplayBuffer): container and methods for storing, adding and organising\n",
    "        experiences\n",
    "        . t_step (int): time-step update counter.\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # 2x Q-networks to enable Fixed Q Targets\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay memory to enable Experience Replay\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size)\n",
    "        \n",
    "        # Timestep counter\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        '''Saves current experience in replay memory, calls 'learn' every 'update_every' time-steps.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . state, action, reward, next_state, done: outputs of a single time-step of agent-environment\n",
    "        interaction.\n",
    "        '''\n",
    "        # Save the current experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn after every 'update_every' steps\n",
    "        self.t_step = (self.t_step + 1) % update_every\n",
    "        \n",
    "        # if 'update_every' time steps have been reached\n",
    "            # and if there are enough samples in memory, retrieve experiences and learn,\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma)\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        '''Selects an epsilon-greedy action.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . state: Agent's current state.\n",
    "        . eps (float): epsilon\n",
    "        '''\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval() # pytorch method for setting a network to evaluation (stop dropout and batchnorm)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        '''Calculates approximate target Q values, current Q values and loss, then does \n",
    "        backpropagation, weight updating for local Q network and calls 'soft_update'.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . experiences (5-tuple): output of agent.memory.sample: tuple of 5 torch float tensors where each\n",
    "        contains respectively k states, actions, rewards, next_states and dones across all experiences\n",
    "        in the batch of k experiences sampled.\n",
    "        . gamma (float): discount factor for reward.\n",
    "        '''\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) #adds a fake batch dimension\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones)) # 1-dones inverts\n",
    "        \n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        '''Updates target q network as a tau-weighted average of local and target model parameters.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . local_model (instance of QNetwork class): local q network.\n",
    "        . target_model (instance of QNetwork class) target q network.\n",
    "        . tau (float): weighting factor for local and target parameters. \n",
    "        '''\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayBuffer class\n",
    "Define a class that the Agent will use to store experiences. Define methods of this class for organising stored experiences, append experiences to memory and retrieve experiences from memory for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        '''Object that instantiates the Agent's memory for Experience Replay and associated methods.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . action_size (int): size of action space.\n",
    "        . buffer_size (int): replay memory capacity; max number of timesteps to store.\n",
    "        . batch_size (int): number of experiences to recall for learning.\n",
    "        \n",
    "        Attributes\n",
    "        ==========\n",
    "        . action_size (int): size of action space.\n",
    "        . storage (deque): storage container for experience tuples.\n",
    "        . batch_size (int): number of experiences to recall for learning.\n",
    "        . experience (function): names a tuple and parses its elements into state, action, reward,\n",
    "        next_state and done.\n",
    "        '''\n",
    "        self.action_size = action_size\n",
    "        self.memory_storage = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        #self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        '''Passes state, action, reward, next_state and done elements into a named tuple (Experience)\n",
    "        and appends them to the memory_storage container.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "        . state, action, reward, next_state, done: outputs of a single time-step of agent-environment\n",
    "        interaction.\n",
    "        '''\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory_storage.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        '''Samples k (=batch_size) Experience tuples, parses and processes them to output a tuple\n",
    "        of 5 torch float tensors each containing respectively k states, actions, rewards, next_states\n",
    "        and dones across the k experiences sampled.        \n",
    "        '''\n",
    "        experiences = random.sample(self.memory_storage, k = self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "Define a function for training the Agent in a Unity environment for a certain episode length and number of timesteps. Training function keeps track of scores per episode and window of 100 episodes, sets a decaying epsilon, uses an episode and timestep loop to control training flow by alternately calling the functions defined above, reports training progress by printing current score, saves the model's learnt parameters to a checkpoint at the end of training and returns the episode-by-episode scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes = 2000, max_t=300, eps_start=1.0, eps_decay=0.995, eps_end=0.005,\n",
    "        brain_name=brain_name, output='checkpoint'):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0] \n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "        \n",
    "            action = int(agent.act(state, eps=eps))\n",
    "            env_info = env.step(action)[brain_name] \n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_end*eps)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode, np.mean(scores_window)), end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "    torch.save(agent.qnetwork_local.state_dict(), '{}.pth'.format(output))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and GPU\n",
    "Pick a device to train on and set hyperparameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Training on {}.'.format(device))\n",
    "\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "lr = 5e-4\n",
    "update_every = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Plot results\n",
    "Instantiate agent & run training function for a chosen number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 2.49\n",
      "Episode 200\tAverage Score: 8.16\n",
      "Episode 289\tAverage Score 10.64"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=s_sz, action_size=a_sz)\n",
    "scores = dqn(n_episodes = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritised experience replay\n",
    "after env.step, but before agent.step,\n",
    "use state, next_state and done to calculate Q_target & Q_expected\n",
    "TD_error = |Q_target - Q_expected|\n",
    "agent.step also saves this to ReplayBuffer\n",
    "Change Replaybuffer structure self.experience to accomodate TD_error\n",
    "change replaybuffer.add to take in TD_error\n",
    "change replaybuffer.sample: need to create a temp priority variable calculated from TD_errors/sum(TD_errors)_over whole buffer\n",
    "Then instead of retrieving experiences = random.sample use np.random.choice with parameter p = this probability array_\n",
    "build in exp parameter a to control rate of reliance on probability dist vs uniform\n",
    "check how to fit in change to update rule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
