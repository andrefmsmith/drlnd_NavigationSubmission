{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Navigation\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Next, we instantiate the Unity Environment for Project 1 (vector observation version of 'Banana'), retrieve the Unity brain name and extract this environment's state size (```s_sz```) and action size (```a_sz```) which we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# start Unity environment\n",
    "env = UnityEnvironment(file_name=\"C:/Users/Andre/Desktop/Udacity DRL/deep-reinforcement-learning/p1_navigation/Banana_Windows_x86_64/Banana.exe\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# get state size and action size\n",
    "#env_info = env.reset(train_mode=False)[brain_name]\n",
    "s_sz = brain.vector_observation_space_size# len(env_info.vector_observations[0])\n",
    "a_sz = brain.vector_action_space_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNetwork, Agent and ReplayBuffer classes, their attributes and methods\n",
    "We are now ready to build an Agent for playing Banana, along with other classes, methods and attributes that will be necessary for implementing a Deep Q Network. Our DQN will use *Fixed Q Targets* and *Experience Replay*.   \n",
    "\n",
    "We start by defining the **QNetwork** class, which will create the fully-connected neural network layers our agent will rely on for learning to approximate Q(s,a) values. **QNetwork** takes arguments for state size, action size and hidden layer width. State size determines the network's input size, and action size determines the size of the network's output.  \n",
    "\n",
    "For this assignment, given the relatively small input space (37-dimensional observation space vector), we will use a 2-hidden layer neural network, with each hidden layer containing 64 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=64):\n",
    "        '''Creates a 2-hidden layer neural network with input and output size defined by\n",
    "        state_size and output_size respectively. Hidden layer width is configured by fc1_units and fc2_units.\n",
    "        \n",
    "        Arguments\n",
    "        =========\n",
    "            state_size (int): size of state space\n",
    "            action_size (int): size of action space\n",
    "            '''\n",
    "        super(QNetwork, self).__init__()\n",
    "        #self.seed = torch.manual_seed(seed) # Sets the seed for generating random numbers\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        '''Performs forward propagation on the QNetwork given a state input, to return action values.'''\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the **Agent** class.  \n",
    "Agent's key attributes are:\n",
    "- **2x instances of QNetwork**: we instantiate two neural networks in order to implement Fixed Q Targets. ```self.qnetwork_local``` is the neural network that learns and stores the weight parameters for approximating the true state-value function. ```self.qnetwork_target``` presents fixed parameter targets for the local network to train up to in each cycle of learning;\n",
    "- A **Memory** store: used to store the agent's experiences and enable offline learning by Experience Replay. This is instantiated by a separate class ```ReplayBuffer``` which we will discuss in more detail below.\n",
    "- A **time step counter**: keeps track of how many time steps have passed in the current experience cycle, enabling the agent to know when to do an update cycle on its local and target QNetworks.  \n",
    "\n",
    "Agent's methods are:\n",
    "- **Step**: saves the current experience to the replay memory, checks if enough time steps have passed for updating QNetworks and if so, calls the functions necessary to learn by Experience Replay (```memory.sample``` and ```self.learn```).\n",
    "- **Act**: given a state, propagates it through the local QNetwork to obtain state-action values for every possible action, then selects an action based on an epsilon-greedy policy.\n",
    "- **Learn**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size):#, seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        #self.seed = random.seed(seed)\n",
    "        \n",
    "        # 2x Q-networks to enable Fixed Q Targets\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        \n",
    "        # Optimizer - notice only used on local qnetwork\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay memory to enable Experience Replay\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size)\n",
    "        \n",
    "        # Timestep counter\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save the current experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn after every 'update_every' steps\n",
    "        self.t_step = (self.t_step + 1) % update_every\n",
    "        \n",
    "        # if 'update_every' time steps have been reached (modulo division above),\n",
    "        if self.t_step == 0:\n",
    "                # and if there are enough samples in memory:\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma)\n",
    "                \n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval() # pytorch method for setting a network to evaluation (stop dropout and batchnorm)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size):#, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        #self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes = 2000, max_t=300, eps_start=1.0, eps_decay=0.995, eps_end=0.005,\n",
    "        brain_name=brain_name, output='checkpoint'):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0] \n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "        \n",
    "            action = int(agent.act(state, eps=eps))\n",
    "            env_info = env.step(action)[brain_name] \n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_end*eps)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode, np.mean(scores_window)), end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "    torch.save(agent.qnetwork_local.state_dict(), '{}.pth'.format(output))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "buffer_size = int(1e6)\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "lr = 5e-4\n",
    "update_every = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.63\n",
      "Episode 200\tAverage Score: 2.72\n",
      "Episode 300\tAverage Score: 7.35\n",
      "Episode 400\tAverage Score: 8.69\n",
      "Episode 500\tAverage Score: 11.30\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=s_sz, action_size=a_sz)\n",
    "scores = dqn(n_episodes = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
