{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is my submission report for Project 1 (Navigation) of Udacity's Deep Reinforcement Learning Nanodegree. In this project, we must train an agent in a Unity environment to collect yellow bananas and avoid blue ones using Deep Q Networks.\n",
    "\n",
    "### Background and Deep Q Network Overview\n",
    "In Deep Q Networks (DQNs), we adopt a model-free approach and use a deep neural network to perform function approximation on the action value function. Briefly, our goal is to learn a function Q'(s, a, w), characterised by the states, actions available in each state and a set of learnable weight parameters w, such that Q' approximates Q(s,a), the true action value function. Function approximation is a powerful approach compared to tabular Q-learning because:\n",
    "1. It enables agents to *generalise*, that is to retrieve action values for states they haven't actually experienced; and,\n",
    "2. It enables agents to tackle *continuous observation spaces*, which would otherwise require infinitely large Q-tables.  \n",
    "\n",
    "Deep Neural Networks are excellent non-linear function approximators but previous attempts to combine them with Reinforcement Learning came into significant challenges, since they were found to diverge or become unstable. There are several causes to this instability, two of which I will highlight here:\n",
    "1. Over learning, sequences of observations can become highly correlated over time-steps. Transitions from one state to the next depend on the action taken by the agent. If an agent learns that a restricted set of actions within a restricted set of states tends to lead to reward, it may well find itself stuck in a loop over transitions involving only those actions and states, which could prevent it from learning the optimal policy; and,\n",
    "2. The fact that the parameters we're tweaking (weights w) to approximate the action value function are the very same parameters we're using to calculate this value. This creates a situation where we have a target that moves as a direct consequence of and in correlation with our attempts to reach it.  \n",
    "\n",
    "The oscillations introduced by these two issues are compounded by the fact that small changes to Q may alter the policy significantly. This is because to generate policy from a Q function, for every state we retrieve the action that maximises value: if Q is unstable even by small margins, our policy could thus swing wildly across evaluation-iteration cycles.  \n",
    "\n",
    "In 2015, [Mnih and colleages](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) published an influential paper aiming to overcome these obstacles in combining Reinforcement Learning with Deep Neural Networks. The resulting approach, termed **Deep Q Networks**, relied on two key ideas:\n",
    "1. **Experience Replay**: learning and experience are dissociated in DQNs. At every time-step, the agent saves its experience to a memory buffer. Every k time-steps, the agent enters a learning epoch wherein it retrieves experience  sequences (S, A, S', R) at random from its memory buffer, which it uses for learning with a temporal difference method. The correlation in experienced sequences is thus broken; and,\n",
    "2. **Fixed Q Targets**: to solve the problem of moving targets, the authors created two neural networks - one for learning parameters to approximate the true Q function, and the other for setting targets for this approximation. Weights for setting targets are calculated and updated separately and at a lag to weights for approximating the Q function, which breaks the correlation between them.  \n",
    "\n",
    "### Algorithm walkthrough\n",
    "**0.** The agent is initialised with 2 neural networks: a local Q Network for learning weight parameters to approximate the true Q function, and a target Q Network for setting approximation targets.  \n",
    "**1.** For a series of time-steps T, the agent observes its state, selects an epsilon-greedy action, and is presented by the environment with a reward (which could be 0) and the next state;  \n",
    "**2.** At the end of each of these time-steps, the agent saves to memory the state, action, reward value and next state resulting from that time-step, in the form of an \"experience\" tuple;  \n",
    "**3.** Every L time-steps, the agent executes a learning sequence:\n",
    "    - Sample K experiences at random from the memory buffer;\n",
    "    - For each experience, use the contents of that experience and i) the target Q Network to estimate the true action value, and ii) the local Q Network to compute its current expectation of action value;\n",
    "    - Calculates loss (minimum square error) between estimated (target) and currently expected (local) action value;\n",
    "    - Based on the discrepancy revealed, update the weights of the local Q Network by gradient descent;\n",
    "    - 'Soft-update' the weights w- of the target Q Network by a constant tau-weighted average between them and the local Q Network's weights w. Target-setting weights w- thus effectively lag in time behind function approximation weights w.\n",
    "**4.** Return to **1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implementation\n",
    "\n",
    "## The QNetwork class\n",
    "\n",
    "We begin by defining a QNetwork class, which instantiates a neural network composed of fully-connected layers according to a pre-defined architecture, where network depth and layer width are specified as arguments.  \n",
    "\n",
    "In tabular approaches to Q learning we learn, store and retrieve *Q values for state-action pairs* from a dictionary or an array. In Deep Q Learning, instead of explicitly learning lookup Q values, we learn the *parameters* of a complex function that returns every action's values when given a state. The function's parameters are stored as the weights of a neural network. Instead of *retrieving* Q values, we *calculate them* for any given state within the state space by passing that state as input to the neural network and computing a forward pass. The key advantages are generalisation and computability for continuous spaces. The drawback is Q values are now highly interdependent, given they're all calculated on the basis of the same function; weight changes that minimise error for a particular region of the function will change the entire function, meaning this approach has some inherent instability.  \n",
    "\n",
    "QNetwork objects are such neural networks that store and learn the weight parameters necessary for estimating Q values when given a state input. Accordingly, their input dimensions match the state size, and their output dimensions match the action size. A forward propagation method is defined such that if an instance of a QNetwork is passed a state, it returns the current estimate of value for each action in that state.  \n",
    "```Python\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent class and its attributes\n",
    "\n",
    "We define an Agent class whose only arguments are the state size and action size:\n",
    "```Python\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        \n",
    "        self.t_step = 0\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        \n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size)\n",
    "        ```\n",
    "**Q Networks** When initialised with a particular state and action size, the Agent class calls QNetwork, passes it state size and action size arguments, and instantiates two QNetworks as attributes: a ```qnetwork_local```, which learns and stores the weights for computing Q values for a given state, and a ```qnetwork_target```, which uses a lagged copy of those weights for computing and setting the Temporal Difference targets for the local QNetwork to learn.  \n",
    "\n",
    "**t_step and optimizer** Agent is endowed with a ```t_step``` attribute which is used as a time-step counter for keeping track of how many time-steps have passed since the last time Agent activated a \"learning\" sequence. In the ```optimizer``` attribute we specify which optimisation method we will use during gradient descent and weight updating, feeding it our desired learning rate hyperparameter.  \n",
    "\n",
    "**Memory and Experience Replay** To enable Experience Replay, we provide the agent with a ```memory``` attribute, which is an instantiation of the ```ReplayBuffer``` class. This class comes with a container for storing experiences and a set of methods for manipulating them:\n",
    "```Python\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.memory_storage = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])```\n",
    "\n",
    "ReplayBuffer allows the agent to store the state, action, reward, next state and episode status for every time-step. We term the collection of all these variables together for the same time step as an \"Experience\" tuple. When we initialise a ```ReplayBuffer``` object, we specify an integer for ```buffer_size```, which defines how many Experience tuples the agent's memory can store, and an integer ```batch_size```, which defines how many Experience tuples the agent should 'recall' from memory when performing a learning episode.  \n",
    "\n",
    "To organise our agent's memory, we create an attribute ```experience``` for the ReplayBuffer class, which we define as a namedtuple object, providing names for it and the fields we expect tuples we pass to it to have.  \n",
    "The great thing about namedtuple object types is that if we define a namedtuple object *a* it acts as a function, such that if we assign a variable *b* to be the result of passing a tuple of appropriate length through namedtuple object *a*:\n",
    "```Python\n",
    "a = namedtuple(\"Experience\", field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "b=a(1,2,0,4,False)```\n",
    "then we can access the values of elements in b by calling them through their field names instead of indexing. For example,\n",
    "```Python\n",
    "b.done```\n",
    "returns False. This will be convenient later on, when we recall ```experiences``` from ```memory``` for learning. We will go over ```ReplayBuffer```'s methods for manipulating ```experiences``` in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent Class and its methods\n",
    "\n",
    "### Storing, manipulating and retrieving Experiences\n",
    "When we train our agent, at every time-step we will call its ```step``` method:\n",
    "```Python\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.t_step = (self.t_step + 1) % update_every\n",
    "        \n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma)```\n",
    "\n",
    "\n",
    "```step``` first calls ```.add```, which is a method of the Agent's ```memory``` and its parent class ReplayBuffer. As the name suggests, ```.add``` is used for adding ```experience``` tuples to the agent's memory storage.\n",
    "```Python\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory_storage.append(e)```\n",
    "\n",
    "Calling ```memory.add``` on a (state, action, reward, next_state, done) tuple passes that tuple to ```memory```'s attribute ```experience```, which in turn organises it as a namedtuple. That namedtuple is then appended to ```memory```'s ```memory_storage``` attribute which is a circular container: a deque of length (capacity) ```buffer_size```. In a circular container such as a deque there is a capacity limit: if adding a new element results in exceeding the deque's capacity, the oldest element is dropped from the beginning of the container, and the newest is added at its end.  \n",
    "\n",
    "Method ```step``` then updates the agent's ```t_step``` counter, resetting it to 0 every time ```update_every``` time-steps have passed. If this condition is met *and* there are enough (>```batch_size```) experience tuples stored in memory, the agent triggers a learning sequence by first calling class ReplayBuffer's method ```.sample```, whose purpose is to retrieve experiences from memory:\n",
    "```Python\n",
    "def sample(self):\n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "```\n",
    "```.sample``` retrieves at random from memory a number ```k = self.batch_size``` of experiences to learn from, then parses the elements of each ```experience``` tuple into a convenient form. So far our data has been organised experience by experience; that is, we have been storing together all the elements that correspond in time to the same time-step's experience. However, during a learning sequence we will want to pass a number ```batch_size``` of states and next states into our local and target Q networks in a form that resembles a mini-batch. It would therefore be desirable to shift to a data structure where we can easily retrieve experience elements of a particular type (say state or next state) *across all the experiences in a mini-batch*.  \n",
    "\n",
    "To achieve this, ```.sample``` takes advantage of the namedtuple data structure. Let's exemplify by looking at states. We use list comprehension of ```e.state``` for e in ```experiences``` to retrieve *only* the state component of each of the k experience tuples stored in the variable ```experiences```. We transform the resulting list into a vertical numpy array by passing it to ```np.vstack```, then turn this into a pytorch tensor, with the ```torch.from_numpy``` method. Finally, we cast it as ```.float()``` and send it to GPU so that it's the appropriate format and place to feed into a neural network for forward propagation. This process is repeated for each of the components of an experience tuple and the return is therefore a tuple of 5 torch tensors where the 'row' index gathers different types of element that correspond to the same experience.  \n",
    "           \n",
    "Returning to ```step```, this output is then passed into method ```learn``` along with the discount rate ```gamma```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from Experiences\n",
    "The function ```learn``` uses data in ```experiences``` and the two QNetworks to update their weight values in order to progressively approximate the action value function.\n",
    "\n",
    "```Python\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau)```\n",
    "        \n",
    "**Setting a Temporal Difference Target** First, ```learn``` unpacks the tuple of elements of ```experiences``` each into its own float tensor. To improve action values outputted by our local QNetwork, we need to provide it with a Temporal Difference (TD) target and check how far the current local network's estimate of value is from this, then update weights to minimise this error. The TD target is the reward obtained for taking action A_t in state S_t plus the discounted value of the max value action at state S_t+1, which is the state taking action A_t led to.  \n",
    "\n",
    "In terms of implementation, we first forward propagate all ```next_states``` in the mini-batch through the target QNetwork. This generates an output of size A where A = action space size, so we take the maximum of this output, thus returning the maximum value that an action could generate at the next state:\n",
    "\n",
    "```Python\n",
    "Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) ```\n",
    "\n",
    "To compute TD target (here instantiated as ```Q_targets```) we need to apply discounting to the max value of the next states, then add received reward values to that:\n",
    "```Python\n",
    "Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))```  \n",
    "\n",
    "If an episode terminates at t + 1 there is no next state and therefore the TD target value should equal reward. An elegant trick to implement this is to invert the tensor of booleans ```dones``` by subtracting it from 1 (True --> False because 1-1=0, False --> True because 1-0=1) then multiplying this by the term ```Q_targets_next```:\n",
    "    - for episodes which finished at the next state, done = True so 1-dones=0, therefore we compute ```Q_targets = rewards + (gamma * Q_targets_next * 0) <=> Q_targets = rewards + 0```;\n",
    "    - for episodes which didn't finish at the next state,  done = False so 1-dones=1, therefore we compute ```Q_targets = rewards + (gamma * Q_targets_next * 1) <=> Q_targets = rewards + gamma * Q_targets_next```.  \n",
    "\n",
    "***Loss, Backprop and Weight Updating*** After setting TD target, we need to compute our local network's current estimation of Q by performing forward propagation of ```states``` through ```qnetwork_local```. The pytorch method ```.gather``` is used to index and retrieve the forward propagation's return by the actions taken as per ```actions``` tensor, meaning we retrieve only the value of those actions. We then compute the mean squared error loss across the whole mini-batch, backpropagate this loss and perform a step of weight updating by gradient descent using our defined optimizer (see above, Adam).  \n",
    "\n",
    "To ensure weights in the target Q Network are not updated, we called the method ```.detach``` on it previously, to take it out of the computation graph. For target network weights, we want to update them slowly so we dampen the correlation between weights used for approximation and target-setting. We do so by calling:\n",
    "```Python\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)```\n",
    "            \n",
    "```soft_update``` performs an average between local and target network parameters, weighted by the constant tau which is a small float, then copies theese weights into the target network, replacing its previous weights. The higher tau is set, the more closely we follow local network weights i.e. the less we dampen the correlation. In the limit, setting tau to 1 would copy over the local network weights exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function\n",
    "\n",
    "Lastly, we implement a training function to call the previous methods and objects in appropriate ways to solve the environment. Function ```dqn``` runs for a ```n_episodes``` number of episodes, and for a ```max_t``` number of time-steps per episode.  \n",
    "\n",
    "At the beginning of each episode, we reset the environment. Then, for each time-step we call ```agent.act``` to select an action, ```env.step(action)``` to perform that action, receive reward and the next state. After saving these values to appropriate variables we call ```agent.step``` to commit the experience tuple to memory and engage the ```learn``` function or not, depending how many time-steps have passed. We move to the next state by assigning next_state to the state variable, and keep track of episode rewards by adding up reward to ```score```.\n",
    "\n",
    "```Python    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0] \n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "        \n",
    "            action = int(agent.act(state, eps=eps))\n",
    "            env_info = env.step(action)[brain_name] \n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break```\n",
    "\n",
    "At the end of an episode, we append the episode's score to our windowed and by-episode tallies, ```scores_window``` and ```scores``` respectively. We then print the current average score based on the last 100 episodes, and leave on display the previous 100 episode average scores.\n",
    "\n",
    "```Python\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_end*eps)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode, np.mean(scores_window)), end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))```\n",
    "\n",
    "dqn has an ```output_toggle``` boolean argument which allows us to decide whether to save a checkpoint at the end of training or not. We can set a filename for a checkpoing by using input argument ```output```. dqn returns only our episode-by-episode tally of scores:\n",
    "```Python\n",
    "   if output_toggle:\n",
    "       torch.save(agent.qnetwork_local.state_dict(), '{}.pth'.format(output))\n",
    "\n",
    "    return scores```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture, hyperparameters and performance\n",
    "### Architecture\n",
    "The default values for my QNetwork class define a **2 hidden layer** architecture, where each layer contains **64 units**. I started with this number due to the relatively low size of the input (state size = 37). The network performed well, so I kept this parameter throughout.\n",
    "\n",
    "I elected also to perform **Batch Normalisation** to normalise the network's input per batch and keep values for the 37 input variables within a similar range. This produced quicker learning and better performance than previous iterations of the network without Batch Normalisation.\n",
    "\n",
    "### Hyperparameters\n",
    "My final submission used the following hyperparameters, and I will offer rationale for these choices below.\n",
    "```Python\n",
    "buffer_size = 300000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "lr = 5e-4\n",
    "update_every = 4```\n",
    "**Buffer Size = 300,000.** Every time step the Agent will store an experience tuple in this buffer. The training function is defined to run each episode of the task for 300 time-steps and I decided to train for 2,000 episodes. This would yield 600,000 experience tuples. Because ```memory``` is defined as a deque, that means adding new elements beyond its defined capacity will remove older elements one by one. I reasoned that by keeping buffer size below the total number of experiences the Agent would encounter, it would progressively 'forget' older experiences, which at later points in training could be less informative for learning.\n",
    "\n",
    "**Batch size = 32.** Batch size defines how many experiences are sampled from memory for learning, setting in effect the mini-batch size for training our neural network, and the minimum number of time-steps that must have elapsed before the agent began learning. Smaller batch sizes will result in noisier estimates of the gradient, whereas larger batches will lead to less frequent weight updating. 32 appeared as a reasonable compromise. I reasoned batch normalisation could also ameliorate some of the variability problems of choosing a smaller batch size.\n",
    "\n",
    "**Gamma = 0.99.** The agent is penalised for collecting blue bananas. In choosing its actions, I would therefore like it to learn to anticipate the consequences of its current actions rather than optimise purely immediate reward, as the latter could lead to it taking actions in the present that result in capturing blue bananas in the future.\n",
    "\n",
    "**Tau = 1e-3.*** Tau is the parameter for weighted averaging between local and target parameters, when soft updating our targets. The higher tau, the less this average weights target parameters. I left this on default setting from the exercises.\n",
    "\n",
    "**Learning rate = 5e-4.** I kept learning rate as default as well. Using batch normalisation suggests I might have gotten away with increasing learning rate, but the network seemed to be solving the task pretty fast already so I opted not to.\n",
    "\n",
    "**Update every = 4.** This defines how often to run the learning sequence. I experimented with 8 and 4 and found better results for 4. In effect that means the network is calling learning episodes twice as often, which made it faster at solving the environment.\n",
    "\n",
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future improvements\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
